import asyncio
import base64
import logging
import traceback
from contextlib import nullcontext
from typing import Optional, Callable, List, Protocol, Any
from uuid import uuid4

from getstream.models import User
from getstream.video import rtc
from getstream.video.rtc import audio_track
from getstream.video.rtc.tracks import SubscriptionConfig, TrackSubscriptionConfig, TrackType

from agents.agents import (
    TransformedVideoTrack,
    Tool,
    PreProcessor,
    TurnDetection,
    ImageProcessor,
    VideoTransformer,
    STT,
    TTS,
    VAD,
    STS, LLM
)

class Processor(Protocol):
    def start(self, data: Any) -> Any:
        """Any initial setup"""
        ...

    def receive_audio(self):
        pass

    def receive_video(self):
        pass

    def state(self):
        """return state for the llm"""
        pass

"""
TODO
- Cleanup Agent class
- Yolo integration
- Text replies

"""

class Agent:
    def __init__(
        self,

        # llm, optionally with sts capabilities
        llm: Optional[LLM] = None,

        # setup stt, tts, and turn detection if not using realtime/sts
        stt: Optional[STT] = None,
        tts: Optional[TTS] = None,
        turn_detection: Optional[TurnDetection] = None,

        # the agent's user info
        agent_user: Optional[User] = None,

        # for video agents. gather data at an interval
        # - roboflow/ yolo typically run continuously
        # - often combined with API calls to fetch stats etc
        # - state from each processor is passed to the LLM
        processors: Optional[List[PreProcessor]] = None,

        # transformers dont keep state/ aren't relevant for the LLM
        # just for applying sound or video effects
        video_transformer: Optional[VideoTransformer] = None,
    ):
        # Create agent user if not provided
        if agent_user is None:
            agent_id = f"agent-{uuid4()}"
            # Create a basic User object with required parameters
            self.agent_user = User(
                id=agent_id, 
                banned=False, 
                online=True, 
                role="user",
                custom={"name": "AI Agent"},
                teams_role={}
            )
        else:
            self.agent_user = agent_user

        self.logger = logging.getLogger(f"Agent[{self.agent_user.id}]")

        self.llm = llm
        self.stt = stt
        self.tts = tts
        self.turn_detection = turn_detection
        self.processors = processors or []
        self.video_transformer = video_transformer
        
        # validation time
        self.validate_configuration()
        self.prepare_rtc()
        self.setup_stt()
        self.create_user()

    def setup_stt(self):
        self.logger.info("üéôÔ∏è Setting up STT event listeners")
        self.stt.on("transcript", self._on_transcript)
        self.stt.on("partial_transcript", self._on_partial_transcript)
        self.stt.on("error", self._on_stt_error)
        self._stt_setup = True

    async def say_text(self, text):
        await self.tts.send(text)

    async def play_audio(self, pcm):
        self._audio_track.send_audio(pcm)

    async def reply_to_text(self, input_text: str):
        '''
        Receive text (from a transcription, or user input)
        Run it through the LLM, get a response. And reply
        '''
        response = await self.llm.generate(input_text)
        await self.say_text(response)

    def get_subscription_config(self):
        return TrackSubscriptionConfig(
            track_types=[
                TrackType.TRACK_TYPE_VIDEO,
                TrackType.TRACK_TYPE_AUDIO,
            ]
        )

    async def join(self, call) -> None:
        """Join a Stream video call."""
        if self._is_running:
            raise RuntimeError("Agent is already running")

        self.logger.info(f"ü§ñ Agent joining call: {call.id}")
        
        if self.sts_mode:
            self.logger.info("üé§ Using STS (Speech-to-Speech) mode")
        else:
            self.logger.info("üé§ Using traditional STT/TTS mode")

        try:
            stsContextManager = None
            if self.sts_mode:
                stsContextManager = await self.llm.connect(call, self.agent_user.id)


            # Traditional mode - use WebRTC connection
            # Configure subscription for audio and video
            subscription_config = SubscriptionConfig(
                default=self.get_subscription_config()
            )

            async with await rtc.join(
                call, self.agent_user.id, subscription_config=subscription_config
            ) as connection, (stsContextManager or nullcontext()) as stsConnection:
                self._connection = connection
                self._sts_connection = stsConnection
                self._is_running = True

                self.logger.info(f"ü§ñ Agent joined call: {call.id}")



                # Set up audio track if available
                if self.publish_audio:
                    await connection.add_tracks(audio=self._audio_track)
                    self.logger.debug("ü§ñ Agent ready to speak")

                # Set up video track if available
                if self.publish_video:
                    await connection.add_tracks(video=self._video_track)
                    self.logger.debug("üé• Agent ready to publish video")

                # Set up STS audio forwarding if in STS mode
                if self.sts_mode and self._sts_connection:
                    self.logger.info("üé• STS audio. Forward from openAI to Stream")
                    await self._setup_sts_audio_forwarding(stsConnection, connection)

                # Set up event handlers for audio processing
                await self.listen_to_audio_and_video()

                # Send initial greeting, if the LLM is configured to do so
                if self.llm:
                    await self.llm.conversation_started(self)

                if self.sts_mode:
                    async def process_sts_events():
                        try:
                            #TODO: some method to receive audio
                            async for event in stsConnection:
                                # also see https://platform.openai.com/docs/api-reference/realtime_server_events/input_audio_buffer/speech_stopped
                                # TODO: implement https://github.com/openai/openai-python/blob/main/examples/realtime/push_to_talk_app.py#L167
                                self.logger.debug(f"üîî STS Event: {event.type}")
                                # Handle any STS-specific events here if needed
                        except Exception as e:
                            self.logger.error(f"‚ùå Error processing STS events: {e}")
                            self.logger.error(traceback.format_exc())

                    # Start STS event processing in background
                    sts_task = asyncio.create_task(process_sts_events())

        except Exception as e:
            self.logger.error(f"‚ùå Error during agent operation: {e}")
            self.logger.error(traceback.format_exc())
            raise
        finally:
            # Use the comprehensive cleanup method
            await self.close()

    async def listen_to_audio_and_video(self) -> None:
        """Set up event handlers for the connection."""
        if not self._connection:
            self.logger.error("‚ùå No active connections found")
            return

        # Handle new participants joining
        async def on_track_published(event):
            try:
                user_id = "unknown"
                if hasattr(event, "participant") and event.participant:
                    user_id = getattr(event.participant, "user_id", "unknown")

                track_id = getattr(event, "track_id", "unknown")
                track_type = getattr(event, "track_type", "unknown")

                self.logger.info(f"üì¢ Track published: {user_id} - {track_id} - {track_type}")

                if user_id and user_id != self.agent_user.id:
                    self.logger.info(f"üëã New participant joined: {user_id}")

            except Exception as e:
                self.logger.error(f"‚ùå Error handling track published event: {e}")
                self.logger.error(traceback.format_exc())

        # Set up WebSocket event handlers
        try:
            if hasattr(self._connection, "_ws_client") and self._connection._ws_client:
                self._connection._ws_client.on_event("track_published", on_track_published)
        except Exception as e:
            self.logger.error(f"Error setting up WebSocket event handlers: {e}")

        # Handle audio data for STT or STS
        @self._connection.on("audio")
        async def on_audio_received(pcm, user):
            await self.reply_to_audio(pcm, user)

        # listen to video tracks
        @self._connection.on("track_added")
        async def on_track(track_id, track_type, user):
            asyncio.create_task(
                self._process_video_track(track_id, track_type, user)
            )

    async def reply_to_audio(self, pcm_data, user) -> None:
        # first forward to processors
        await self._forward_audio_to_processors(pcm_data, user)
        # when in STS mode call the STS directly
        if self.sts_mode:
            self.llm.send_audio(pcm_data, user)
        else:
            # Process audio through STT
            self.logger.debug(f"üéµ Processing audio from {user}")
            await self.stt.process_audio(pcm_data, user)


    async def _forward_audio_to_processors(self, pcm_data, user) -> None:
        """Forward audio data to processors that want to receive audio."""
        if not self.processors:
            return

        try:
            # Extract audio bytes from PCM data
            audio_bytes = None
            if hasattr(pcm_data, 'samples'):
                # PcmData NamedTuple - extract samples (numpy array)
                samples = pcm_data.samples
                if hasattr(samples, 'tobytes'):
                    audio_bytes = samples.tobytes()
                else:
                    # Convert numpy array to bytes
                    audio_bytes = samples.astype('int16').tobytes()
            elif isinstance(pcm_data, bytes):
                # Already bytes
                audio_bytes = pcm_data
            else:
                self.logger.error(f"Unknown PCM data format: {type(pcm_data)}")
                return

            # Forward to processors that want audio
            for processor in self.processors:
                if hasattr(processor, 'receive_audio') and processor.receive_audio:
                    try:
                        user_id = user if isinstance(user, str) else getattr(user, 'user_id', str(user))
                        await processor.process_audio(audio_bytes, user_id)
                    except Exception as e:
                        self.logger.error(f"Error forwarding audio to processor {type(processor).__name__}: {e}")

        except Exception as e:
            self.logger.error(f"Error in audio forwarding: {e}")
            self.logger.error(traceback.format_exc())

    async def _forward_video_to_processors(self, frame, user) -> None:
        """Forward video frame to processors that want to receive video."""
        if not self.processors:
            return

        try:
            from PIL import Image
            import numpy as np
            
            self.logger.debug(f"üé• Processing video frame of type: {type(frame)}")
            
            pil_image = None
            
            # Try to convert different frame formats to PIL Image
            if isinstance(frame, Image.Image):
                # Already a PIL Image
                pil_image = frame
            elif hasattr(frame, 'to_ndarray'):
                # VideoFrame from aiortc - convert to numpy array then PIL
                try:
                    array = frame.to_ndarray(format='rgb24')
                    pil_image = Image.fromarray(array)
                    self.logger.debug(f"‚úÖ Converted VideoFrame to PIL Image: {pil_image.size}")
                except Exception as e:
                    self.logger.error(f"Error converting VideoFrame to PIL: {e}")
                    return
            elif hasattr(frame, 'to_image'):
                # Some other video frame format
                try:
                    pil_image = frame.to_image()
                    self.logger.debug(f"‚úÖ Converted frame to PIL Image using to_image(): {pil_image.size}")
                except Exception as e:
                    self.logger.error(f"Error converting frame using to_image(): {e}")
                    return
            elif isinstance(frame, np.ndarray):
                # Numpy array - convert to PIL
                try:
                    pil_image = Image.fromarray(frame)
                    self.logger.debug(f"‚úÖ Converted numpy array to PIL Image: {pil_image.size}")
                except Exception as e:
                    self.logger.error(f"Error converting numpy array to PIL: {e}")
                    return
            else:
                self.logger.warning(f"‚ö†Ô∏è Unknown video frame format: {type(frame)}. Available methods: {[method for method in dir(frame) if not method.startswith('_')]}")
                return

            if pil_image:
                # Forward to processors that want video
                video_processors = [p for p in self.processors if hasattr(p, 'receive_video') and p.receive_video]
                self.logger.debug(f"üì§ Forwarding video frame to {len(video_processors)} processors")
                
                for processor in video_processors:
                    try:
                        user_id = user if isinstance(user, str) else getattr(user, 'user_id', str(user))
                        await processor.process_image(pil_image, user_id)
                    except Exception as e:
                        self.logger.error(f"Error forwarding video to processor {type(processor).__name__}: {e}")
                        self.logger.error(traceback.format_exc())

        except Exception as e:
            self.logger.error(f"Error in video forwarding: {e}")
            self.logger.error(traceback.format_exc())

    async def _process_video_track(self, track_id: str, track_type: str, user):
        """Process video frames from a specific track."""
        self.logger.info(
            f"üé• Processing video track: {track_id} from user {user.user_id} (type: {track_type})"
        )

        # Only process video tracks
        if track_type != "video":
            self.logger.debug(f"Ignoring non-video track: {track_type}")
            return

        # Subscribe to the video track
        track = self._connection.subscriber_pc.add_track_subscriber(track_id)
        if not track:
            self.logger.error(f"‚ùå Failed to subscribe to track: {track_id}")
            return

        self.logger.info(
            f"‚úÖ Successfully subscribed to video track from {user.user_id}"
        )

        try:
            while True:
                try:
                    # Receive video frame
                    video_frame = await track.recv()
                    if not video_frame:
                        continue

                    # Convert to PIL Image
                    img = video_frame.to_image()
                    self.logger.debug(f"üì∏ Converted video frame to PIL Image: {img.size}")

                    # Forward to processors that want video
                    await self._forward_video_to_processors(img, user)

                except Exception as e:
                    if "Connection closed" in str(e) or "Track ended" in str(e):
                        self.logger.info(
                            f"üîå Video track ended for user {user.user_id}"
                        )
                        break
                    else:
                        self.logger.error(f"‚ùå Error processing video frame: {e}")
                        self.logger.error(traceback.format_exc())
                        await asyncio.sleep(1)  # Brief pause before retry

        except Exception as e:
            self.logger.error(f"‚ùå Fatal error in video processing: {e}")
            self.logger.error(traceback.format_exc())

    async def _on_partial_transcript(self, text: str, user=None, metadata=None):
        """Handle partial transcript from STT service."""
        if text and text.strip():
            user_info = user.user_id if user and hasattr(user, "user_id") else "unknown"
            self.logger.debug(f"üé§ [{user_info}] (partial): {text}")

    async def _on_transcript(self, text: str, user=None, metadata=None):
        """Handle final transcript from STT service."""
        if text and text.strip():
            user_info = user.user_id if user and hasattr(user, "user_id") else "unknown"
            self.logger.info(f"üé§ [{user_info}]: {text}")

            # Process transcription through LLM and respond
            await self._process_transcription(text, user)

    async def _on_stt_error(self, error):
        """Handle STT service errors."""
        self.logger.error(f"‚ùå STT Error: {error}")

    async def _process_transcription(self, text: str, user=None) -> None:
        """Process a complete transcription and generate response."""
        try:
            # Generate response using LLM
            if self.llm:
                # TODO: async version of this
                response = await self._generate_response(text)

                # Send response via TTS
                if self.tts and response:
                    try:
                        await self.tts.send(response)
                        self.logger.info(f"ü§ñ Responded: {response}")
                    except Exception as e:
                        self.logger.error(f"Error sending TTS response: {e}")
                        self.logger.error(traceback.format_exc())

        except Exception as e:
            self.logger.error(f"Error processing transcription: {e}")
            self.logger.error(traceback.format_exc())

    async def _generate_response(self, input_text: str) -> str:
        """Generate a response using the AI model."""
        try:
            response = await self.llm.generate(input_text)
            return response
        except Exception as e:
            self.logger.error(f"Error generating response: {e}")
            self.logger.error(traceback.format_exc())
            return "I'm sorry, I encountered an error processing your request."

    @property
    def sts_mode(self) -> bool:
        """Check if the agent is in STS mode."""
        if self.llm is None:
            return False
        return getattr(self.llm, 'sts', False)
    
    @property
    def publish_audio(self) -> bool:
        if self.tts is not None or self.sts_mode:
            return True
        else:
            return False
        
    @property
    def publish_video(self) -> bool:
        if self.video_transformer is not None:
            return True
        else:
            return False

    def validate_configuration(self):
        """Validate the agent configuration."""
        if self.sts_mode:
            # STS mode - should not have separate STT/TTS
            if self.stt or self.tts:
                self.logger.warning(
                    "STS mode detected: STT and TTS services will be ignored. "
                    "The STS model handles both speech-to-text and text-to-speech internally."
                )
        else:
            # Traditional mode - need LLM and either STT/TTS or both
            if not self.llm:
                raise ValueError("LLM is required for traditional mode")
            if not self.stt and not self.tts:
                raise ValueError("At least one of STT or TTS is required for traditional mode")

    def prepare_rtc(self):
        # Initialize common variables
        self._current_frame = None
        self._interval_task = None
        self._is_running = False
        self._callback_executed = False

        self._connection: Optional[rtc.RTCConnection] = None
        self._audio_track: Optional[audio_track.AudioStreamTrack] = None
        self._video_track: Optional[TransformedVideoTrack] = None

        # Set up audio track if TTS is available
        if self.publish_audio:
            self._audio_track = audio_track.AudioStreamTrack(framerate=16000)
            if self.tts:
                self.tts.set_output_track(self._audio_track)

        # Set up video track if video transformer is available
        if self.publish_video:
            self._video_track = TransformedVideoTrack()
            self.logger.info("üé• Video track initialized for transformation publishing")


    async def _setup_sts_audio_forwarding(self, sts_connection, rtc_connection):
        """Set up audio forwarding from STS connection to WebRTC connection."""
        self.logger.info("üîó Setting up STS audio forwarding")
        
        # Set up audio forwarding from STS to WebRTC
        if self._audio_track:
            async def forward_sts_audio(audio_data):
                """Forward audio from STS connection to WebRTC connection."""
                try:
                    self.logger.info(f"üéµ Forwarding {len(audio_data)} bytes of STS audio to WebRTC")
                    # Send audio data to the audio track
                    await self._audio_track.send_audio(audio_data)
                    self.logger.debug("‚úÖ Audio forwarded successfully")
                except Exception as e:
                    self.logger.error(f"Error forwarding STS audio: {e}")
                    self.logger.error(traceback.format_exc())
            
            # Check which type of STS connection we have
            if hasattr(sts_connection, 'on_audio') and callable(getattr(sts_connection, 'on_audio')):
                # Gemini Live-style connection - register audio callback
                sts_connection.on_audio(forward_sts_audio)
                self.logger.info("‚úÖ Gemini Live audio forwarding configured")
            elif hasattr(sts_connection, 'on_audio'):
                # OpenAI-style connection with on_audio decorator
                @sts_connection.on_audio
                async def forward_openai_audio(audio_data):
                    await forward_sts_audio(audio_data)
                self.logger.info("‚úÖ OpenAI audio forwarding configured")
            else:
                self.logger.warning("‚ö†Ô∏è STS connection doesn't support audio forwarding")
        else:
            self.logger.warning("‚ö†Ô∏è Audio track not available for forwarding")

    async def close(self):
        """Clean up all connections and resources."""
        self._is_running = False
        
        if self._sts_connection:
            await self._sts_connection.__aexit__(None, None, None)
            self._sts_connection = None
        
        if self._connection:
            await self._connection.__aexit__(None, None, None)
            self._connection = None
        
        if self.stt:
            await self.stt.close()
        
        if self.tts:
            await self.tts.close()
        
        if self._audio_track:
            self._audio_track.stop()
            self._audio_track = None
        
        if self._video_track:
            self._video_track.stop()
            self._video_track = None
        
        if self._interval_task:
            self._interval_task.cancel()
            self._interval_task = None

    def create_user(self):
        """Create user - placeholder for any user setup logic."""
        pass
